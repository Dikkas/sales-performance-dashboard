{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sales Data Preparation for Power BI Dashboard\n",
        "\n",
        "This project focuses on preparing sales data from the **Global Superstore dataset** using Python.  \n",
        "The data cleaning and transformation steps are performed in Google Colab, and the processed data will be exported for visualization and reporting in Power BI.\n",
        "\n",
        "The quality of data is fundamental for accurate analysis and effective strategic decision-making based on business dashboards. We aim to produce clean, reliable, and well-structured data to support insightful business dashboards.\n",
        "\n",
        "**Dataset**: Global Superstore (Kaggle version)  \n",
        "**Author**: Paulo Castro  \n",
        "**Date**: July 2025  \n",
        "**Tools**: Python (pandas), Power BI\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LQa7VnVCeFea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Loading and Inspecting the Data\n",
        "\n",
        "In this section, we load the dataset and perform an initial inspection to understand its structure and basic characteristics.\n",
        "\n",
        "We focus on:\n",
        "- The number of rows and columns\n",
        "- Data types of each column\n",
        "- General completeness of the data\n",
        "\n",
        "> **Note**: The file was imported using `encoding='latin1'` due to character encoding issues with special characters in the original CSV file."
      ],
      "metadata": {
        "id": "Z8SYP3ltec7_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9NgZ1Pxrmjd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/Global_Superstore2.csv', encoding='latin1')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Initial Data Overview from `df.info()`\n",
        "\n",
        "Upon loading the dataset and examining its structure using `df.info()`, we observe the following key characteristics:\n",
        "\n",
        "* **Total Entries**: The dataset comprises `51290` entries, indicating a substantial volume of transactional data.\n",
        "* **Columns**: There are `24` columns, each representing a specific attribute of the sales transactions.\n",
        "* **Data Types**:\n",
        "    * **Object Type**: `17` columns are of `object` type. Notably, `Order Date` and `Ship Date` are currently `object` type and will require conversion to `datetime` objects for proper time-series analysis.\n",
        "    * **Numeric Types**: `5` columns are `float64` and `2` are `int64`, primarily representing quantitative measures like `Sales`, `Profit`, `Discount`, `Shipping Cost`, and `Quantity`.\n",
        "* **Non-Null Counts**:\n",
        "    * The `Postal Code` column shows a significant number of missing values (`9994` non-null out of `51290`), suggesting a potential lack of postal information for many entries.\n",
        "    * All other critical analytical columns (e.g., `Sales`, `Profit`, `Quantity`, `Order ID`) are fully populated (`51290` non-null), which is ideal for subsequent analysis.\n",
        "\n",
        "This initial inspection highlights the need for data type conversion and flags the `Postal Code` column for potential handling of missing values."
      ],
      "metadata": {
        "id": "xFINjvPb5SiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "nv8MmF5_8Uw7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Snapshot of Raw Data from `df.head()`\n",
        "\n",
        "Inspecting the first five rows of the DataFrame with `df.head()` provides a quick visual understanding of the data content and format:\n",
        "\n",
        "* **Granularity**: Each row appears to represent a specific item within an order, showing details like `Product Name`, `Sales`, `Quantity`, `Discount`, and `Profit` per item.\n",
        "* **Date Format**: The `Order Date` and `Ship Date` columns visually confirm their current string format (e.g., '31-07-2012'), reinforcing the necessity for `datetime` conversion.\n",
        "* **Key Variables**: We can clearly see the presence of important categorical variables like `Ship Mode`, `Segment`, `Category`, and `Sub-Category`, which will be crucial for segmentation and categorization in our analysis and Power BI dashboard.\n",
        "* **Numeric Values**: The `Sales`, `Profit`, `Discount`, and `Shipping Cost` columns display a variety of numerical values, giving an initial sense of the range and distribution of these financial metrics."
      ],
      "metadata": {
        "id": "F4-s0KW75Wvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "MOTjcrmzR_0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Cleaning and Preparation\n",
        "\n",
        "In this section, we perform data cleaning to ensure the dataset is suitable for analysis and dashboarding.  \n",
        "The main goals are:\n",
        "\n",
        "- Converting date columns to proper datetime format  \n",
        "- Optimizing categorical columns to reduce memory usage  \n",
        "- Removing duplicated records  \n",
        "- Reviewing and handling missing values (if applicable)\n",
        "\n",
        "This will result in a clean and structured dataset ready for Power BI integration."
      ],
      "metadata": {
        "id": "fBlo-Kgle6JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Converting Date Columns\n",
        "\n",
        "We convert the `Order Date` and `Ship Date` columns from string to datetime format.  \n",
        "The parameter `dayfirst=True` is used because the dataset follows the `DD-MM-YYYY` format."
      ],
      "metadata": {
        "id": "dqP5ZgZb9bm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Order Date'] = pd.to_datetime(df['Order Date'], dayfirst=True)\n",
        "df['Ship Date'] = pd.to_datetime(df['Ship Date'], dayfirst=True)\n",
        "\n",
        "# Verify the conversion\n",
        "df[['Order Date', 'Ship Date']].info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7AXfoMQb0O1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Optimizing Categorical Columns\n",
        "\n",
        "To improve memory efficiency and speed up future processing, we convert all object-type columns with **20 or fewer unique values** to the `category` data type."
      ],
      "metadata": {
        "id": "N95ryPfl9gvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_cat = df.select_dtypes(include='object').nunique().sort_values()\n",
        "cols_cat = cols_cat[cols_cat <= 20].index.tolist()\n",
        "\n",
        "df[cols_cat] = df[cols_cat].astype('category')\n",
        "df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "koeMj0NT9hdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Removing Duplicated Rows\n",
        "\n",
        "We identify and remove duplicated rows from the dataset to avoid data redundancy and inaccurate aggregations."
      ],
      "metadata": {
        "id": "2vb8PW5d-bst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rows before removing duplicates:\", df.shape[0])\n",
        "print(\"Duplicated rows found:\", df.duplicated().sum())\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Rows after removing duplicates:\", df.shape[0])"
      ],
      "metadata": {
        "id": "yukeXBEC5VMU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Missing Values Check\n",
        "\n",
        "To ensure data completeness and reliability for our analysis, we will check for missing values across all columns. Special attention will be given to the main numeric columns and KPIs: `Sales`, `Profit`, `Shipping Cost`, `Discount`, and `Quantity`."
      ],
      "metadata": {
        "id": "jl2nxC1t-ljE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['Sales', 'Profit', 'Shipping Cost', 'Discount', 'Quantity']].isnull().sum())"
      ],
      "metadata": {
        "id": "cxYvz2vysbH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Summary Statistics for Numerical Columns\n",
        "\n",
        "We examine basic descriptive statistics for key numerical fields: `Sales`, `Profit`, `Shipping Cost`, `Discount`, and `Quantity`.\n",
        "\n",
        "This provides a foundational understanding of value ranges and distributions before proceeding to outlier analysis."
      ],
      "metadata": {
        "id": "Pea5HUhT--UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Sales', 'Profit', 'Shipping Cost', 'Discount', 'Quantity']].describe()"
      ],
      "metadata": {
        "id": "xkFCfeuz7Ztr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5.1 Interpretation of Numerical Statistics\n",
        "\n",
        "The descriptive statistics provide critical initial insights into the distribution and range of our key numerical variables, highlighting important data characteristics:\n",
        "\n",
        "* **Sales**: Exhibit a very wide range, from a minimum of `0.44` to a maximum of `22638.48`, with a mean of `246.49`. The absence of negative values for `Sales` suggests there are no erroneous entries or recorded returns in this column, indicating a clean transaction record. The high maximum value, significantly deviating from the mean, indicates the presence of exceptionally large sales, which may be **outliers** to be further investigated.\n",
        "* **Profit**: Shows a substantial spread, ranging from a minimum of `-6599.98` (losses) to a maximum of `8399.98`. While the mean profit is `28.61`, the presence of negative values confirms transactions resulting in financial losses. Similar to Sales, the extremely high absolute values (both positive and negative) suggest the presence of **outliers** in Profit, warranting closer examination.\n",
        "* **Shipping Cost**: Ranges from `0.0` to `933.57` with a mean of `26.38`. The minimum of `0.0` is plausible for certain shipping agreements. Like Sales, the absence of negative shipping costs suggests data integrity.\n",
        "* **Discount**: Varies from `0.0` to `0.85` (85%), with an average of `0.14` (14%). The absence of negative discounts is expected, as discounts are reductions.\n",
        "* **Quantity**: Shows a reasonable distribution from `1` to `14` units, with a mean of `3.48`. The minimum of `1` and maximum of `14` are within expected bounds for order quantities.\n",
        "\n",
        "These statistics provide strong indicators of data quality (e.g., no negative sales/quantity) and identify potential areas of interest (e.g., high-value transactions and losses in `Sales` and `Profit`) that warrant a dedicated **outlier analysis**, as will be performed in the next steps."
      ],
      "metadata": {
        "id": "CC-QfJwA_OKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Detecting Outliers (IQR Method)\n",
        "\n",
        "We define a function to detect outliers using the Interquartile Range (IQR) method, which is a common technique to identify extreme values in continuous distributions.\n",
        "\n",
        "Outliers are flagged in the following columns:\n",
        "\n",
        "- `Sales`\n",
        "- `Profit`\n",
        "- `Shipping Cost`"
      ],
      "metadata": {
        "id": "v1K97v4RC3bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect outliers using the IQR method\n",
        "def detect_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] < lower_bound) | (df[column] > upper_bound)]"
      ],
      "metadata": {
        "id": "aQK0kWHTVkVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify outliers in Sales\n",
        "outliers_sales = detect_outliers(df, 'Sales')\n",
        "print(f'Outliers in Sales: {outliers_sales.shape[0]}')\n",
        "\n",
        "# Identify outliers in Profit\n",
        "outliers_profit = detect_outliers(df, 'Profit')\n",
        "print(f'Outliers in Profit: {outliers_profit.shape[0]}')\n",
        "\n",
        "# Identify outliers in Shipping Cost\n",
        "outliers_shipping_cost = detect_outliers(df, 'Shipping Cost')\n",
        "print(f'Outliers in Shipping Cost: {outliers_shipping_cost.shape[0]}')"
      ],
      "metadata": {
        "id": "7YUmT2-GLdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 Correlation Matrix\n",
        "\n",
        "To understand potential linear relationships, we compute the Pearson correlation between the following variables:\n",
        "\n",
        "- `Sales`\n",
        "- `Profit`\n",
        "- `Shipping Cost`\n",
        "- `Discount`\n",
        "\n",
        "This will help validate whether some outliers can be explained by expected interactions (e.g., high discounts reducing profit)."
      ],
      "metadata": {
        "id": "J9J_4-adC93x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df[['Sales', 'Profit','Shipping Cost', 'Discount']].corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "q0Nn-JwlM39i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.7.1 Interpretation of the Correlation Matrix\n",
        "\n",
        "The correlation matrix reveals several important relationships between the selected variables:\n",
        "* **`Sales` and `Shipping Cost` (0.768)**: There is a strong positive correlation, indicating that transactions with higher sales volumes generally imply higher shipping costs.\n",
        "* **`Sales` and `Profit` (0.485)**: A moderate positive correlation, suggesting that, in general, higher sales contribute to higher profits.\n",
        "* **`Profit` and `Discount` (-0.316)**: A moderate negative correlation. This is a crucial point, as it confirms the hypothesis that applying discounts tends to negatively impact profit margins. This relationship is expected and helps contextualize some of the `Profit` outliers (losses) previously identified.\n",
        "* The correlations between `Discount` and `Sales` (-0.087), and `Discount` and `Shipping Cost` (-0.079) are very weak, indicating that the discount alone does not have a strong direct linear relationship with sales volume or shipping costs in this data.\n",
        "\n",
        "This correlation analysis provides a statistical understanding of the interactions between variables, which will be complemented by visual exploration."
      ],
      "metadata": {
        "id": "zHIJLLWyFCSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8 Scatter Plots for Visual Exploration\n",
        "\n",
        "We generate scatter plots to visually explore the impact of `Discount` on:\n",
        "\n",
        "- `Sales`\n",
        "- `Profit`\n",
        "\n",
        "This step helps us understand whether the statistical outliers detected earlier are justified by real business dynamics.\n"
      ],
      "metadata": {
        "id": "wX4mvxndDA8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot of Sales vs Discount\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Discount', y='Sales', data=df)\n",
        "plt.title('Sales vs Discount')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot of Profit vs Discount\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Discount', y='Profit', data=df)\n",
        "plt.title('Profit vs Discount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6c5kGleNP-Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.9 Why Outliers Were Not Removed\n",
        "\n",
        "During initial exploration, an IQR-based method identified up to **21,319 records** (≈41% of the dataset) as potential outliers.  \n",
        "The most extreme case was the `Profit` column, with **9,755 flagged entries**.\n",
        "\n",
        "Such high volumes suggest that these values are **not necessarily errors**, but could reflect realistic, rare events — such as:\n",
        "\n",
        "- Bulk orders\n",
        "- High-margin or loss-leader products\n",
        "- Logistics-driven pricing\n",
        "\n",
        "To support this hypothesis, we analyzed the **correlation matrix** including `Discount`, revealing:\n",
        "\n",
        "|                | Sales     | Profit    | Shipping Cost | Discount   |\n",
        "|----------------|-----------|-----------|----------------|------------|\n",
        "| **Sales**      | 1.000     | 0.485     | 0.768          | -0.087     |\n",
        "| **Profit**     | 0.485     | 1.000     | 0.354          | -0.316     |\n",
        "| **Shipping**   | 0.768     | 0.354     | 1.000          | -0.079     |\n",
        "| **Discount**   | -0.087    | -0.316    | -0.079         | 1.000      |\n",
        "\n",
        "We also visually inspected scatterplots and identified several data points that are distant from the norm. For example:\n",
        "\n",
        "- At `Discount = 0.50`, most sales are below 5,000 — but one value exceeds **22,000**, possibly indicating a data entry error or a rare high-value sale.\n",
        "\n",
        "However, because these cases do not distort the overall trends and may reflect **genuine market behavior**, we opted to:\n",
        "\n",
        "**Acknowledge these as visual outliers**,  \n",
        "**But keep them in the dataset** to preserve completeness and business realism.\n",
        "\n",
        "In Power BI, such values can be flagged with visual cues or filters, allowing stakeholders to explore them as needed."
      ],
      "metadata": {
        "id": "rDjmN4x0C26i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exporting Clean Data for Power BI\n",
        "\n",
        "After completing the data cleaning process, we export the dataset to a `.csv` file for use in Power BI.\n",
        "\n",
        "The exported dataset includes:\n",
        "\n",
        "- Cleaned and deduplicated sales records  \n",
        "- Formatted datetime fields (`Order Date`, `Ship Date`)  \n",
        "- Categorical columns optimized  \n",
        "- No rows removed due to outliers — they were reviewed but retained for transparency\n",
        "\n",
        "The exported file will serve as the data source for our dashboards.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u9mCsW-GWQwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export clean data to CSV (ready for Power BI)\n",
        "df.to_csv('/content/Global_Superstore_Clean.csv', index=False)"
      ],
      "metadata": {
        "id": "wq3XWRurWRP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Power BI Dashboard Plan\n",
        "\n",
        "The visualization layer of this project will be developed in **Power BI**, using the cleaned dataset.  \n",
        "The dashboard will be structured into two thematic pages for clarity and impact."
      ],
      "metadata": {
        "id": "N9XaoJ6HYpN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page 1 – Sales Overview\n",
        "\n",
        "This page will focus on general business performance and sales trends.\n",
        "\n",
        "**Main visuals:**\n",
        "\n",
        "- **KPI Cards**:  \n",
        "  - Total Sales  \n",
        "  - Total Profit  \n",
        "  - Average Discount  \n",
        "\n",
        "- **Line Chart**:  \n",
        "  - Sales and Profit by Month\n",
        "\n",
        "- **Bar Charts**:  \n",
        "  - Sales by Segment  \n",
        "  - Sales by Category  \n",
        "\n",
        "- **Column Chart**:\n",
        "  - Top 10 Sub-Categories by Sales\n",
        "\n",
        "- **Filters (Slicers):**  \n",
        "  - Order Year / Month  \n",
        "  - Region\n",
        "  - Segment\n",
        "  - Category"
      ],
      "metadata": {
        "id": "Nf7gTdQhYtwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page 2 – Customer & Order Behavior\n",
        "\n",
        "This page will provide insights into order dynamics and customer behavior.\n",
        "\n",
        "**Main visuals:**\n",
        "\n",
        "- **KPI Cards:**  \n",
        "  - Total Orders  \n",
        "  - Total Distinct Customers  \n",
        "  - Average Shipping Delay\n",
        "\n",
        "- **Column Chart:**  \n",
        "  - Quantity by Sub-Category\n",
        "\n",
        "- **Scatter Plot:**  \n",
        "  - Profit vs. Discount\n",
        "\n",
        "- **Stacked Bar Chart:**  \n",
        "  - Sales by Region and Category\n",
        "\n",
        "- **Table (for outlier review):**  \n",
        "  - Order ID\n",
        "  - Customer Name\n",
        "  - Sales\n",
        "  - Discount\n",
        "  - Profit\n"
      ],
      "metadata": {
        "id": "UP7_N8R-Yz_W"
      }
    }
  ]
}